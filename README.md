# rnn_translation_enfr

## ğŸ“Œ Contexte
Projet acadÃ©mique rÃ©alisÃ© dans le cadre du TP notÃ© **Traduction Automatique** (L3 Intelligence Artificielle, UniversitÃ© CÃ´te dâ€™Azur).  
Objectif : entraÃ®ner un modÃ¨le RNN encodeurâ€“dÃ©codeur pour traduire des phrases de lâ€™anglais vers le franÃ§ais, en explorant diffÃ©rents paramÃ¨tres dâ€™apprentissage et lâ€™intÃ©gration dâ€™un tokenizer **SentencePiece**.

---

## ğŸ› ï¸ MÃ©thodologie
- ImplÃ©mentation dâ€™un **RNN caractÃ¨re-par-caractÃ¨re** (encoder + decoder).  
- ExpÃ©riences avec la taille du dataset, du nombre dâ€™Ã©poques et des hyperparamÃ¨tres (`embed_size`, `hidden_size`).  
- IntÃ©gration dâ€™un **tokenizer SentencePiece BPE** (16k vocabulaire, tokens spÃ©ciaux).  
- EntraÃ®nement sur un sous-ensemble du dataset Kaggle (~20 000 paires).  
- Optimiseur : **Adam** avec taux dâ€™apprentissage ajustÃ© (0.001 â†’ 0.0003).  
- Fonction de perte : **CrossEntropyLoss** avec masquage du padding.  

---

## ğŸ“Š RÃ©sultats
- **Ã‰volution de la perte** : 6.01 â†’ 0.44 (40 Ã©poques).  
- **Top-1 Accuracy** â‰ˆ 25.5 %  
- **Top-5 Accuracy** â‰ˆ 26.4 %  
- Observations :  
  - La loss diminue fortement â†’ apprentissage effectif.  
  - Mais les traductions restent souvent incohÃ©rentes â†’ limites dâ€™un RNN simple.  
  - Lâ€™augmentation de `embed_size` et `hidden_size` amÃ©liore les scores (+5 Ã  10 %), sans dÃ©passer 30 %.  
  - ExpÃ©rimentation Beam Search â†’ non concluant.  

---

##  Auteur
Projet rÃ©alisÃ© par Rostom Samar
ğŸ”— github.com/rostomsamar
